{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iQjAkGVEeq41"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel, BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.decomposition import PCA\n",
        "import re\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.layers import Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Subtract, Concatenate\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYJS0nT_-kFm",
        "outputId": "f01dfecb-19d0-4de7-bb22-17cf6614c9c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "\n",
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r0VQXB2Od7C7",
        "outputId": "839b10a1-5523-4490-a9b3-9c3ea67229da"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweets"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a2f1d52f-617b-4d70-872c-22e29c81deca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>303056</th>\n",
              "      <td>0</td>\n",
              "      <td>Hockey will be ending soon. That makes me sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199034</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan @etherjammer Yeah~  I have *terrible...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226310</th>\n",
              "      <td>0</td>\n",
              "      <td>Life is always COMPLICATED!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598410</th>\n",
              "      <td>0</td>\n",
              "      <td>@lilandtedsmum well at least spuds and grapes ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158535</th>\n",
              "      <td>0</td>\n",
              "      <td>@fadedmoon owww ppl are voting for suarez, fra...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2f1d52f-617b-4d70-872c-22e29c81deca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a2f1d52f-617b-4d70-872c-22e29c81deca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a2f1d52f-617b-4d70-872c-22e29c81deca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-546e49aa-91cd-4882-b292-487dc2a9b365\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-546e49aa-91cd-4882-b292-487dc2a9b365')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-546e49aa-91cd-4882-b292-487dc2a9b365 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        target                                               text\n",
              "303056       0     Hockey will be ending soon. That makes me sad \n",
              "199034       0  @Kenichan @etherjammer Yeah~  I have *terrible...\n",
              "226310       0                      Life is always COMPLICATED!! \n",
              "598410       0  @lilandtedsmum well at least spuds and grapes ...\n",
              "158535       0  @fadedmoon owww ppl are voting for suarez, fra..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets = pd.read_csv('/tweets.csv', encoding='cp1252', header=None)\n",
        "tweets.columns = ['target','id','date','flag','username','text'] #Change column names to things that make sense\n",
        "tweets = tweets.drop(columns=['id','date','flag','username']) #Remove unneeded columns from memory\n",
        "tweets = tweets.replace({'target':{0:0,4:1}}) #Dataset has only 0=negative sent, 4=positive sent, remappping to 0,1 respectivly\n",
        "# print(tweets.shape)\n",
        "tweets = tweets.groupby('target').sample(250000,random_state=None)\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Clean the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ku5cbMkrefvU",
        "outputId": "bc8a8d9a-3b61-4627-8d1a-9f859680a1b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweets"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2d7ea376-73d1-4902-8764-c2c7da876295\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>303056</th>\n",
              "      <td>0</td>\n",
              "      <td>Hockey will be ending soon. That makes me sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199034</th>\n",
              "      <td>0</td>\n",
              "      <td>Kenichan etherjammer Yeah I have terrible anxi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226310</th>\n",
              "      <td>0</td>\n",
              "      <td>Life is always COMPLICATED!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598410</th>\n",
              "      <td>0</td>\n",
              "      <td>lilandtedsmum well at least spuds and grapes a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158535</th>\n",
              "      <td>0</td>\n",
              "      <td>fadedmoon owww ppl are voting for suarez, fran...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d7ea376-73d1-4902-8764-c2c7da876295')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d7ea376-73d1-4902-8764-c2c7da876295 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d7ea376-73d1-4902-8764-c2c7da876295');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-724c8fe5-7ff4-4a8c-b726-3215ca4da071\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-724c8fe5-7ff4-4a8c-b726-3215ca4da071')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-724c8fe5-7ff4-4a8c-b726-3215ca4da071 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        target                                               text\n",
              "303056       0      Hockey will be ending soon. That makes me sad\n",
              "199034       0  Kenichan etherjammer Yeah I have terrible anxi...\n",
              "226310       0                       Life is always COMPLICATED!!\n",
              "598410       0  lilandtedsmum well at least spuds and grapes a...\n",
              "158535       0  fadedmoon owww ppl are voting for suarez, fran..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_tweet(tweet):\n",
        "    # Remove any non-alphabetic characters except basic punctuation\n",
        "    tweet = re.sub(r\"[^a-zA-Z0-9.,'!? ]\", '', tweet)\n",
        "    # Remove any excess whitespace\n",
        "    tweet = re.sub(r\"\\s+\", ' ', tweet).strip()\n",
        "    return tweet\n",
        "\n",
        "tweets['text'] = tweets['text'].apply(clean_tweet)\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-AjKYKQegzn",
        "outputId": "f598afc5-1cd9-43df-d589-00105cef1ba5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "target\n",
              "0    250000\n",
              "1    250000\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets.value_counts('target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMHfdUjrhW7z",
        "outputId": "6cc67f80-40b9-4006-8c20-8b220b94a48b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(400000,) (100000,) (400000,) (100000,)\n"
          ]
        }
      ],
      "source": [
        "#train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['text'], tweets['target'], test_size=0.2, random_state=265)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-u6CvQRsLQb",
        "outputId": "432db563-8be3-44c6-ce64-9ccc724a98b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valgirl omg I really hope thats not hte case!!\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "#print the first item in X_train and y_train\n",
        "print(X_train.iloc[0])\n",
        "print(y_train.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn6cC6FxeRUp",
        "outputId": "19db0d0c-f531-4bd4-9eda-d4562251f821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 287386 unique tokens. Distilled to 287386 top words.\n",
            "Shape of data tensor: (400000, 165)\n",
            "Shape of label tensor: (400000,)\n",
            "287386\n",
            "CPU times: user 20.4 s, sys: 232 ms, total: 20.7 s\n",
            "Wall time: 20.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "NUM_TOP_WORDS = None # use entire vocabulary!\n",
        "MAX_ART_LEN = np.max([len(tweet) for tweet in tweets['text']]) # maximum number of words in a tweet\n",
        "\n",
        "#tokenize the text\n",
        "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# save as sequences with integers replacing words\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
        "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
        "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_ART_LEN)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_ART_LEN)\n",
        "\n",
        "\n",
        "#y_ohe = keras.utils.to_categorical(y_train, num_classes=2)\n",
        "print('Shape of data tensor:', X.shape)\n",
        "print('Shape of label tensor:', y_train.shape)\n",
        "print(np.max(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCNz82dpnVvk",
        "outputId": "930a6a52-6953-49a1-9a6f-4cc443a2b6b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 165, 300)          86216100  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 163, 64)           57664     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 32, 64)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 64)            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 2049      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86275813 (329.12 MB)\n",
            "Trainable params: 86275813 (329.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 53s 31ms/step - loss: 0.5464 - accuracy: 0.7208 - val_loss: 0.5245 - val_accuracy: 0.7387\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.4071 - accuracy: 0.8157 - val_loss: 0.5717 - val_accuracy: 0.7255\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 20s 13ms/step - loss: 0.2696 - accuracy: 0.8789 - val_loss: 0.6603 - val_accuracy: 0.7155\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 18s 12ms/step - loss: 0.2052 - accuracy: 0.9065 - val_loss: 0.7552 - val_accuracy: 0.7066\n",
            "3125/3125 [==============================] - 6s 2ms/step - loss: 0.7552 - accuracy: 0.7066\n",
            "Test accuracy: 0.7066100239753723\n",
            "Embedding weights shape: (287387, 300)\n"
          ]
        }
      ],
      "source": [
        "# Parameters for embedding layer\n",
        "VOCAB_SIZE = len(word_index) + 1  # Add 1 for padding\n",
        "EMBED_SIZE = 300  # You can change this size according to your preference\n",
        "\n",
        "# Create model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, input_length=MAX_ART_LEN))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "#make learning rate in adam optimizer smaller\n",
        "adam_optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "#early stopping from val_loss\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "model.compile(optimizer= 'adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y_train, epochs=10, batch_size=256, validation_split=0.2, validation_data=(X_test, y_test), callbacks=early_stopping)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Once the model is trained, you can retrieve the learned embedding weights\n",
        "embedding_weights = model.layers[0].get_weights()[0]\n",
        "print(\"Embedding weights shape:\", embedding_weights.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0l8TyKEZ8cJk"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "#ran this to save the model\n",
        "\n",
        "# MODEL_PATH = './drive/My Drive/Colab/CNNEmbeddingTrainer.h5'\n",
        "\n",
        "# # Now save model in drive\n",
        "# model.save(MODEL_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz9YpwmqEL-D",
        "outputId": "e19ca119-ff01-4283-cdca-a40494d593ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 165, 300)          86216100  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 163, 64)           57664     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 32, 64)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32, 64)            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 2049      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86275813 (329.12 MB)\n",
            "Trainable params: 86275813 (329.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Embedding weights shape: (287387, 300)\n"
          ]
        }
      ],
      "source": [
        "# # Load Model\n",
        "# model3 = load_model(MODEL_PATH)\n",
        "# model3.summary()\n",
        "\n",
        "# embedding_weights = model3.layers[0].get_weights()[0]\n",
        "# print(\"Embedding weights shape:\", embedding_weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "raAWMbjRHm16"
      },
      "outputs": [],
      "source": [
        "#np.save('/content/drive/My Drive/Colab/embedding_weights.npy', embedding_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLcD65LnB_5i",
        "outputId": "cf2799e6-a47d-4287-c5be-66f29a9f6aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 287387 word vectors.\n",
            "Embedding Shape: (287387, 300) \n",
            " Total words found: 287386 \n",
            " Percentage: 99.99965203714851\n"
          ]
        }
      ],
      "source": [
        "EMBED_SIZE = 300\n",
        "print('Found %s word vectors.' % len(embedding_weights))\n",
        "\n",
        "# Now fill in the matrix using the ordering from the keras word tokenizer\n",
        "found_words = 0\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_weights[i]  # Use the learned embeddings\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        found_words += 1\n",
        "\n",
        "print(\"Embedding Shape:\", embedding_matrix.shape, \"\\n\",\n",
        "      \"Total words found:\", found_words, \"\\n\",\n",
        "      \"Percentage:\", 100 * found_words / embedding_matrix.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r8wjCjCKCN1J"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# save this embedding now\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "    EMBED_SIZE,\n",
        "    weights=[embedding_matrix], # here is the embedding getting saved\n",
        "    input_length=MAX_ART_LEN,\n",
        "    trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pxaxciwKa-n-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
        "from tensorflow.keras.layers import Subtract\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "\n",
        "# The transformer architecture\n",
        "class TransformerBlock(Layer): # inherit from Keras Layer\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
        "        super().__init__()\n",
        "        # setup the model heads and feedforward network\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embed_dim)\n",
        "\n",
        "        # make a two layer network that processes the attention\n",
        "        self.ffn = Sequential()\n",
        "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
        "        self.ffn.add( Dense(embed_dim) )\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # apply the layers as needed (similar to PyTorch)\n",
        "\n",
        "        # get the attention output from multi heads\n",
        "        # Using same inpout here is self-attention\n",
        "        # call inputs are (query, value, key)\n",
        "        # if only two inputs given, value and key are assumed the same\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "\n",
        "        # create residual output, with attention\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # apply dropout if training\n",
        "        out1 = self.dropout1(out1, training=training)\n",
        "\n",
        "        # place through feed forward after layer norm\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        # apply dropout if training\n",
        "        out2 = self.dropout2(out2, training=training)\n",
        "        #return the residual from Dense layer\n",
        "        return out2\n",
        "\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # create two embeddings\n",
        "        # one for processing the tokens (words)\n",
        "        self.token_emb = Embedding(input_dim=vocab_size,\n",
        "                                   output_dim=embed_dim)\n",
        "        # another embedding for processing the position\n",
        "        self.pos_emb = Embedding(input_dim=maxlen,\n",
        "                                 output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        # create a static position measure (input)\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
        "        positions = self.pos_emb(positions)# embed these positions\n",
        "        x = self.token_emb(x) # embed the tokens\n",
        "        return x + positions # add embeddngs to get final embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r0KHx7MDHv33"
      },
      "outputs": [],
      "source": [
        "class CustomTokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # create two embeddings\n",
        "        # one for processing the tokens (words)\n",
        "        self.token_emb = Embedding(len(word_index)+1,\n",
        "                                   EMBED_SIZE,\n",
        "                                   weights=[embedding_matrix],\n",
        "                                   input_length=MAX_ART_LEN)\n",
        "        # another embedding for processing the position\n",
        "        self.pos_emb = Embedding(MAX_ART_LEN,\n",
        "                                 EMBED_SIZE,\n",
        "                                 input_length=MAX_ART_LEN,\n",
        "                                )\n",
        "\n",
        "    def call(self, x):\n",
        "        # create a static position measure (input)\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
        "        positions = self.pos_emb(positions)# embed these positions\n",
        "        x = self.token_emb(x) # embed the tokens\n",
        "        return x + positions # add embeddngs to get final embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Transformer using our previous project's CNN to train embedding of dataset\n",
        "#### 2 Heads, 128 Neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWvax_v7IgPb",
        "outputId": "981c2827-4487-4731-8b5e-5caa2a6fbf8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 162)]             0         \n",
            "                                                                 \n",
            " custom_token_and_position_  (None, 162, 300)          86652900  \n",
            " embedding_4 (CustomTokenAn                                      \n",
            " dPositionEmbedding)                                             \n",
            "                                                                 \n",
            " transformer_block_6 (Trans  (None, 162, 300)          742832    \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " global_average_pooling1d_4  (None, 300)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 300)               0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 128)               38528     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87434389 (333.54 MB)\n",
            "Trainable params: 87434389 (333.54 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "3125/3125 [==============================] - 118s 37ms/step - loss: 0.3388 - accuracy: 0.8582 - val_loss: 0.5653 - val_accuracy: 0.7725\n",
            "Epoch 2/30\n",
            "3125/3125 [==============================] - 98s 31ms/step - loss: 0.2692 - accuracy: 0.8898 - val_loss: 0.5830 - val_accuracy: 0.7650\n",
            "Epoch 3/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.2504 - accuracy: 0.8988 - val_loss: 0.6257 - val_accuracy: 0.7662\n",
            "Epoch 4/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.2388 - accuracy: 0.9041 - val_loss: 0.6611 - val_accuracy: 0.7626\n",
            "Epoch 5/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.2296 - accuracy: 0.9081 - val_loss: 0.6650 - val_accuracy: 0.7646\n",
            "Epoch 6/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.2206 - accuracy: 0.9117 - val_loss: 0.6852 - val_accuracy: 0.7639\n",
            "Epoch 7/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.2132 - accuracy: 0.9148 - val_loss: 0.6969 - val_accuracy: 0.7642\n",
            "Epoch 8/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.2068 - accuracy: 0.9172 - val_loss: 0.6826 - val_accuracy: 0.7618\n",
            "Epoch 9/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.2009 - accuracy: 0.9197 - val_loss: 0.7083 - val_accuracy: 0.7582\n",
            "Epoch 10/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1963 - accuracy: 0.9216 - val_loss: 0.7235 - val_accuracy: 0.7616\n",
            "Epoch 11/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1919 - accuracy: 0.9234 - val_loss: 0.7257 - val_accuracy: 0.7643\n",
            "Epoch 12/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1873 - accuracy: 0.9251 - val_loss: 0.7357 - val_accuracy: 0.7626\n",
            "Epoch 13/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1829 - accuracy: 0.9266 - val_loss: 0.7542 - val_accuracy: 0.7581\n",
            "Epoch 14/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1794 - accuracy: 0.9280 - val_loss: 0.7480 - val_accuracy: 0.7616\n",
            "Epoch 15/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1757 - accuracy: 0.9296 - val_loss: 0.7844 - val_accuracy: 0.7542\n",
            "Epoch 16/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1724 - accuracy: 0.9306 - val_loss: 0.7702 - val_accuracy: 0.7597\n",
            "Epoch 17/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1691 - accuracy: 0.9324 - val_loss: 0.7862 - val_accuracy: 0.7597\n",
            "Epoch 18/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1661 - accuracy: 0.9335 - val_loss: 0.7822 - val_accuracy: 0.7561\n",
            "Epoch 19/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1631 - accuracy: 0.9346 - val_loss: 0.7971 - val_accuracy: 0.7599\n",
            "Epoch 20/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1607 - accuracy: 0.9355 - val_loss: 0.8411 - val_accuracy: 0.7558\n",
            "Epoch 21/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1580 - accuracy: 0.9364 - val_loss: 0.8277 - val_accuracy: 0.7555\n",
            "Epoch 22/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1556 - accuracy: 0.9375 - val_loss: 0.8401 - val_accuracy: 0.7571\n",
            "Epoch 23/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1533 - accuracy: 0.9384 - val_loss: 0.8328 - val_accuracy: 0.7543\n",
            "Epoch 24/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1504 - accuracy: 0.9395 - val_loss: 0.8669 - val_accuracy: 0.7553\n",
            "Epoch 25/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.1490 - accuracy: 0.9402 - val_loss: 0.8750 - val_accuracy: 0.7548\n",
            "Epoch 26/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.1466 - accuracy: 0.9413 - val_loss: 0.8699 - val_accuracy: 0.7541\n",
            "Epoch 27/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.1444 - accuracy: 0.9422 - val_loss: 0.9047 - val_accuracy: 0.7538\n",
            "Epoch 28/30\n",
            "3125/3125 [==============================] - 97s 31ms/step - loss: 0.1428 - accuracy: 0.9424 - val_loss: 0.9328 - val_accuracy: 0.7506\n",
            "Epoch 29/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1414 - accuracy: 0.9430 - val_loss: 0.9218 - val_accuracy: 0.7506\n",
            "Epoch 30/30\n",
            "3125/3125 [==============================] - 96s 31ms/step - loss: 0.1389 - accuracy: 0.9440 - val_loss: 0.9226 - val_accuracy: 0.7505\n"
          ]
        }
      ],
      "source": [
        "# def transformer_custom\n",
        "\n",
        "embed_dim = 300  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "top_words = 100\n",
        "NUM_CLASSES =  1\n",
        "\n",
        "adam_optimizer = Adam(learning_rate=1e-5)\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "x = CustomTokenAndPositionEmbedding(X_train.shape[1], top_words, embed_dim)(inputs)\n",
        "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
        "              kernel_initializer='glorot_uniform')(x)\n",
        "xformer = Model(inputs=inputs, outputs=outputs)\n",
        "model_xformer = Model(inputs=inputs, outputs=outputs)\n",
        "print(model_xformer.summary())\n",
        "xformer.compile(optimizer=adam_optimizer,\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "history = xformer.fit(\n",
        "    X_train, y_train, batch_size=128, epochs=30,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Transformer using our previous project's CNN to train embedding of dataset\n",
        "#### 3 Heads, 128 Neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEeemj9T5LCd",
        "outputId": "70f2b75e-abe3-4b3c-d7b0-b18fe8b7f0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 165)]             0         \n",
            "                                                                 \n",
            " custom_token_and_position_  (None, 165, 300)          86265600  \n",
            " embedding (CustomTokenAndP                                      \n",
            " ositionEmbedding)                                               \n",
            "                                                                 \n",
            " transformer_block (Transfo  (None, 165, 300)          1103732   \n",
            " rmerBlock)                                                      \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 300)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 300)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               38528     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87407989 (333.44 MB)\n",
            "Trainable params: 87407989 (333.44 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 181s 57ms/step - loss: 0.3765 - accuracy: 0.8355 - val_loss: 0.5423 - val_accuracy: 0.7676\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 123s 39ms/step - loss: 0.3227 - accuracy: 0.8629 - val_loss: 0.5665 - val_accuracy: 0.7659\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 122s 39ms/step - loss: 0.3043 - accuracy: 0.8727 - val_loss: 0.5990 - val_accuracy: 0.7630\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 121s 39ms/step - loss: 0.2910 - accuracy: 0.8793 - val_loss: 0.6153 - val_accuracy: 0.7617\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 121s 39ms/step - loss: 0.2797 - accuracy: 0.8848 - val_loss: 0.6115 - val_accuracy: 0.7603\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 121s 39ms/step - loss: 0.2696 - accuracy: 0.8895 - val_loss: 0.6150 - val_accuracy: 0.7600\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 121s 39ms/step - loss: 0.2614 - accuracy: 0.8926 - val_loss: 0.6575 - val_accuracy: 0.7621\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 120s 39ms/step - loss: 0.2538 - accuracy: 0.8959 - val_loss: 0.6553 - val_accuracy: 0.7620\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 121s 39ms/step - loss: 0.2473 - accuracy: 0.8986 - val_loss: 0.6507 - val_accuracy: 0.7603\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 121s 39ms/step - loss: 0.2409 - accuracy: 0.9014 - val_loss: 0.6688 - val_accuracy: 0.7581\n"
          ]
        }
      ],
      "source": [
        "# def transformer_custom\n",
        "\n",
        "embed_dim = 300  # Embedding size for each token\n",
        "num_heads = 3  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "top_words = 100\n",
        "NUM_CLASSES =  1\n",
        "\n",
        "adam_optimizer = Adam(learning_rate=1e-5)\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "x = CustomTokenAndPositionEmbedding(X_train.shape[1], top_words, embed_dim)(inputs)\n",
        "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
        "              kernel_initializer='glorot_uniform')(x)\n",
        "xformer = Model(inputs=inputs, outputs=outputs)\n",
        "model_xformer = Model(inputs=inputs, outputs=outputs)\n",
        "print(model_xformer.summary())\n",
        "xformer.compile(optimizer=adam_optimizer,\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "history = xformer.fit(\n",
        "    X_train, y_train, batch_size=128, epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer using Keras Embedding Layer\n",
        "#### 2 heads 20 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaoJfa5ea-n-",
        "outputId": "e88a73ba-d74a-4d8c-be16-90892709eda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 162)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, 162, 32)           8384      \n",
            " ng_1 (TokenAndPositionEmbe                                      \n",
            " dding)                                                          \n",
            "                                                                 \n",
            " transformer_block_3 (Trans  (None, 162, 32)           10656     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " global_average_pooling1d_1  (None, 32)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 20)                660       \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 20)                0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 21        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19721 (77.04 KB)\n",
            "Trainable params: 19721 (77.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "top_words = 100\n",
        "NUM_CLASSES =  1\n",
        "\n",
        "inputs = Input(shape=(X_train.shape[1],))\n",
        "x = TokenAndPositionEmbedding(X_train.shape[1], top_words, embed_dim)(inputs)\n",
        "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(20, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
        "              kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "model_xformer = Model(inputs=inputs, outputs=outputs)\n",
        "print(model_xformer.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nljGguxsa-n-",
        "outputId": "f86dd53e-553a-4369-fc3e-13cbbf4708e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3125/3125 [==============================] - 54s 16ms/step - loss: 0.6455 - accuracy: 0.6029 - val_loss: 0.6087 - val_accuracy: 0.6635\n",
            "Epoch 2/10\n",
            "3125/3125 [==============================] - 23s 7ms/step - loss: 0.6104 - accuracy: 0.6646 - val_loss: 0.6069 - val_accuracy: 0.6638\n",
            "Epoch 3/10\n",
            "3125/3125 [==============================] - 23s 7ms/step - loss: 0.6086 - accuracy: 0.6654 - val_loss: 0.6064 - val_accuracy: 0.6638\n",
            "Epoch 4/10\n",
            "3125/3125 [==============================] - 22s 7ms/step - loss: 0.6068 - accuracy: 0.6662 - val_loss: 0.6036 - val_accuracy: 0.6669\n",
            "Epoch 5/10\n",
            "3125/3125 [==============================] - 22s 7ms/step - loss: 0.6054 - accuracy: 0.6673 - val_loss: 0.6036 - val_accuracy: 0.6677\n",
            "Epoch 6/10\n",
            "3125/3125 [==============================] - 22s 7ms/step - loss: 0.6042 - accuracy: 0.6683 - val_loss: 0.6005 - val_accuracy: 0.6690\n",
            "Epoch 7/10\n",
            "3125/3125 [==============================] - 22s 7ms/step - loss: 0.6032 - accuracy: 0.6700 - val_loss: 0.6015 - val_accuracy: 0.6693\n",
            "Epoch 8/10\n",
            "3125/3125 [==============================] - 22s 7ms/step - loss: 0.6022 - accuracy: 0.6697 - val_loss: 0.5992 - val_accuracy: 0.6714\n",
            "Epoch 9/10\n",
            "3125/3125 [==============================] - 21s 7ms/step - loss: 0.6012 - accuracy: 0.6710 - val_loss: 0.5987 - val_accuracy: 0.6715\n",
            "Epoch 10/10\n",
            "3125/3125 [==============================] - 22s 7ms/step - loss: 0.6007 - accuracy: 0.6714 - val_loss: 0.5999 - val_accuracy: 0.6725\n"
          ]
        }
      ],
      "source": [
        "# optimizer = Adam(learning_rate=1e-5)\n",
        "model_xformer.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "history = model_xformer.fit(\n",
        "    X_train, y_train, batch_size=128, epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1Z9sKd4a-n_"
      },
      "outputs": [],
      "source": [
        "#model.get_layer(*name u get from model.summary*)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
