{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(16000,)\n",
      "(16000,)\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Loading and Sampling the desired layer\n",
    "tweets = pd.read_csv('../Data/tweets.csv', encoding='cp1252', header=None)\n",
    "tweets.columns = ['target','id','date','flag','username','text'] #Change column names to things that make sense\n",
    "tweets = tweets.drop(columns=['id','date','flag','username']) #Remove unneeded columns from memory\n",
    "tweets = tweets.replace({'target':{0:0,4:1}}) #Dataset has only 0=negative sent, 4=positive sent, remappping to 0,1 respectivly\n",
    "tweets = tweets.groupby('target').sample(10000,random_state=None)\n",
    "\n",
    "# Tokenizing the data\n",
    "# features = tokenizer(tweets['text'].values.tolist(), padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "# features = tweets['text'].apply(lambda x: tokenizer(x.lower(), padding=True, truncation=True, max_length=128, return_tensors='tf'))\n",
    "# features = bert_model(**features).last_hidden_state[:,0,:]\n",
    "# print(\"ONE FEATURE ASPECT\")\n",
    "# # print(features[14:15, :])\n",
    "# tweets['text'] = features.numpy().tolist()\n",
    "\n",
    "#split data into train and test sets\n",
    "# X = np.array([x for x in tweets['text']])\n",
    "# Y = np.array([x for x in tweets['target']])\n",
    "X = tweets.drop('target', axis=1)\n",
    "Y = tweets['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets['text'], tweets['target'], test_size=0.2, random_state=265)\n",
    "\n",
    "#reduce dimensionality\n",
    "#try to increase feature size later\n",
    "# pca = PCA(n_components=3)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.fit_transform(X_test)\n",
    "\n",
    "# X_train = (X_train - np.min(X_train))/(np.max(X_train) - np.min(X_train))\n",
    "# X_test = (X_test - np.min(X_test))/(np.max(X_test) - np.min(X_test))\n",
    "\n",
    "print(type(X_train))\n",
    "# print(X_train.head)\n",
    "# print(y_train.head)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26727 unique tokens. Distilled to 26727 top words.\n"
     ]
    }
   ],
   "source": [
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_ART_LEN = np.max([len(tweet) for tweet in tweets['text']]) # maximum number of words in a tweet\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_ART_LEN)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_ART_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "# The transformer architecture \n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, \n",
    "                                      key_dim=embed_dim)\n",
    "        \n",
    "        # make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # apply the layers as needed (similar to PyTorch)\n",
    "        \n",
    "        # get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # call inputs are (query, value, key) \n",
    "        # if only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        \n",
    "        # create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "        \n",
    "        # place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "    \n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings \n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, \n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 219)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 219, 32)          967008    \n",
      " g_10 (TokenAndPositionEmbed                                     \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " transformer_block_10 (Trans  (None, 219, 32)          10656     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d_10  (None, 32)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_486 (Dropout)       (None, 32)                0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_487 (Dropout)       (None, 20)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 978,345\n",
      "Trainable params: 978,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "top_words = 30000\n",
    "NUM_CLASSES =  1\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = TokenAndPositionEmbedding(X_train.shape[1], top_words, embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model_xformer = Model(inputs=inputs, outputs=outputs)\n",
    "print(model_xformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 40s 155ms/step - loss: 0.6962 - accuracy: 0.5058 - val_loss: 0.6935 - val_accuracy: 0.4990\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.6938 - accuracy: 0.5076 - val_loss: 0.6934 - val_accuracy: 0.5010\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.6705 - accuracy: 0.5569 - val_loss: 0.5478 - val_accuracy: 0.7232\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.4731 - accuracy: 0.7854 - val_loss: 0.5017 - val_accuracy: 0.7605\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.3073 - accuracy: 0.8700 - val_loss: 0.5807 - val_accuracy: 0.7455\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.1721 - accuracy: 0.9359 - val_loss: 0.7628 - val_accuracy: 0.7350\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.0910 - accuracy: 0.9698 - val_loss: 1.0937 - val_accuracy: 0.7207\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.0563 - accuracy: 0.9824 - val_loss: 1.3543 - val_accuracy: 0.7170\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.0419 - accuracy: 0.9869 - val_loss: 1.3747 - val_accuracy: 0.7170\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.0294 - accuracy: 0.9901 - val_loss: 1.7202 - val_accuracy: 0.7085\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=1e-7)\n",
    "model_xformer.compile(optimizer='adam', \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history = model_xformer.fit(\n",
    "    X_train, y_train, batch_size=64, epochs=10, \n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_21_layer_call_fn, embedding_21_layer_call_and_return_conditional_losses, embedding_22_layer_call_fn, embedding_22_layer_call_and_return_conditional_losses, multi_head_attention_10_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_model/assets\n"
     ]
    }
   ],
   "source": [
    "model_xformer.save(\"transformer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 5s 36ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# yhat_cnn = model_xformer.predict(X_test)\n",
    "yhat_xformer = model_xformer.predict(X_test)\n",
    "\n",
    "acc = [mt.accuracy_score(y_test,np.round(yhat_xformer))\n",
    "      ]\n",
    "\n",
    "plt.bar([1],acc)\n",
    "plt.xticks([1],['XFORMER1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_layer(*name u get from model.summary*)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
