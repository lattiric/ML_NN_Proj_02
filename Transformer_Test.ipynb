{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n",
      "(16, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizer, TFBertModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tweets = pd.read_csv('../Data/tweets.csv', encoding='cp1252', header=None)\n",
    "tweets.columns = ['target','id','date','flag','username','text'] #Change column names to things that make sense\n",
    "tweets = tweets.drop(columns=['id','date','flag','username']) #Remove unneeded columns from memory\n",
    "tweets = tweets.replace({'target':{0:0,4:1}}) #Dataset has only 0=negative sent, 4=positive sent, remappping to 0,1 respectivly\n",
    "# print(tweets.shape)\n",
    "tweets = tweets.groupby('target').sample(10,random_state=None)\n",
    "# print(tweets.shape)\n",
    "\n",
    "# print(tweets.head())\n",
    "features = tokenizer(tweets['text'].values.tolist(), padding='max_length', truncation=True, max_length=99, return_tensors='tf')\n",
    "# tweets['text'] = tweets['text'].apply(lambda x: tokenizer(x.lower(), padding=True, truncation=True, max_length=128, return_tensors='tf'))\n",
    "features = bert_model(**features).last_hidden_state[:,0,:]\n",
    "# print(\"ONE FEATURE ASPECT\")\n",
    "# # print(features[14:15, :])\n",
    "\n",
    "tweets['text'] = features.numpy().tolist()\n",
    "# print(tweets.head())\n",
    "\n",
    "X = np.array([x for x in tweets['text']])\n",
    "Y = np.array([x for x in tweets['target']])\n",
    "# X = tweets.drop('target', axis=1)\n",
    "# Y = tweets['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=265)\n",
    "\n",
    "#try to increase feature size later\n",
    "pca = PCA(n_components=3)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "# The transformer architecture \n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, \n",
    "                                      key_dim=embed_dim)\n",
    "        \n",
    "        # make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # apply the layers as needed (similar to PyTorch)\n",
    "        \n",
    "        # get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # call inputs are (query, value, key) \n",
    "        # if only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        \n",
    "        # create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "        \n",
    "        # place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "    \n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings \n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, \n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 3, 32)            3296      \n",
      " g_9 (TokenAndPositionEmbedd                                     \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block_9 (Transf  (None, 3, 32)            10656     \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d_9   (None, 32)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_3331 (Dropout)      (None, 32)                0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_3332 (Dropout)      (None, 20)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,633\n",
      "Trainable params: 14,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "top_words = 100\n",
    "NUM_CLASSES =  1\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = TokenAndPositionEmbedding(X_train.shape[1], top_words, embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model_xformer = Model(inputs=inputs, outputs=outputs)\n",
    "print(model_xformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_8/token_and_position_embedding_9/embedding_18/embedding_lookup' defined at (most recent call last):\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/traitlets/config/application.py\", line 978, in launch_instance\n      app.start()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/hx/z5bls9fd24g4pfrwpcmmf25m0000gn/T/ipykernel_25987/836710900.py\", line 5, in <module>\n      history = model_xformer.fit(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/hx/z5bls9fd24g4pfrwpcmmf25m0000gn/T/ipykernel_25987/909682129.py\", line 71, in call\n      x = self.token_emb(x) # embed the tokens\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/layers/core/embedding.py\", line 199, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model_8/token_and_position_embedding_9/embedding_18/embedding_lookup'\nindices[1,0] = -1 is not in [0, 100)\n\t [[{{node model_8/token_and_position_embedding_9/embedding_18/embedding_lookup}}]] [Op:__inference_train_function_340166]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [179], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model_xformer\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                       loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                       metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m model_xformer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      6\u001b[0m     X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m      7\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test)\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow_2.9/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_8/token_and_position_embedding_9/embedding_18/embedding_lookup' defined at (most recent call last):\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/traitlets/config/application.py\", line 978, in launch_instance\n      app.start()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/hx/z5bls9fd24g4pfrwpcmmf25m0000gn/T/ipykernel_25987/836710900.py\", line 5, in <module>\n      history = model_xformer.fit(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/hx/z5bls9fd24g4pfrwpcmmf25m0000gn/T/ipykernel_25987/909682129.py\", line 71, in call\n      x = self.token_emb(x) # embed the tokens\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/ricklattin/tensorflow_2.9/lib/python3.8/site-packages/keras/layers/core/embedding.py\", line 199, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model_8/token_and_position_embedding_9/embedding_18/embedding_lookup'\nindices[1,0] = -1 is not in [0, 100)\n\t [[{{node model_8/token_and_position_embedding_9/embedding_18/embedding_lookup}}]] [Op:__inference_train_function_340166]"
     ]
    }
   ],
   "source": [
    "model_xformer.compile(optimizer='adam', \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history = model_xformer.fit(\n",
    "    X_train, y_train, batch_size=64, epochs=2, \n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m mt\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# yhat_cnn = model_xformer.predict(X_test)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m yhat_xformer1 \u001b[38;5;241m=\u001b[39m model_xformer\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# yhat_cnn = model_xformer.predict(X_test)\n",
    "yhat_xformer1 = model_xformer.predict(X_test)\n",
    "yhat_xformer2 = model_xformer.predict(X_test)\n",
    "\n",
    "acc = [mt.accuracy_score(y_test,np.round(yhat_xformer1)),\n",
    "       mt.accuracy_score(y_test,np.round(yhat_xformer2)),\n",
    "      ]\n",
    "\n",
    "plt.bar([1,2],acc)\n",
    "plt.xticks([1,2],['XFORMER1','XFORMER2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_layer(*name u get from model.summary*)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
